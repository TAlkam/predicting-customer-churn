{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyOOBfIPqHiiu5dIoj0CdDt7",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/TAlkam/predicting-customer-churn/blob/main/Team_Project_predicting_customer_churn.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Tursun Alkam"
      ],
      "metadata": {
        "id": "dJsgeYQ9HZXg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **1. Introduction**\n",
        "\n",
        "**1.1 Project Goal**\n",
        "The goal of this project is to predict customer churn using a machine learning model. Customer churn prediction helps businesses identify customers who are likely to stop using their services, enabling them to take proactive measures to retain these customers and reduce losses.\n",
        "\n",
        "\n",
        "**1.2 Importance of Churn Prediction**\n",
        "Customer churn is a significant issue for many businesses, particularly in subscription-based industries. Predicting churn allows businesses to understand which customers are at risk of leaving and to implement strategies to retain them, thereby increasing customer lifetime value and overall profitability.\n",
        "\n",
        "\n",
        "**1.3 Business Understanding**\n",
        "Objective: Reduce customer churn to increase revenue and improve customer retention.\n",
        "\n",
        "\n",
        "**Business Need:** The retail business needs a model to predict which customers are likely to churn so that targeted marketing strategies can be implemented to retain them.\n",
        "\n",
        "\n",
        "# **2. Data Understanding**\n",
        "\n",
        "**2.1 Find Data**\n",
        "We used a publicly available dataset: \"Customer Churn Dataset\" from Kaggle.\n",
        "\n",
        "\n",
        "2.2 Examine Data\n",
        "Load the Dataset: Load the dataset and inspect the columns and data types.\n",
        "Identify the Target Variable: The target variable is 'Churn', and the features include customer demographics, purchase history, and other relevant attributes.\n",
        "\n",
        "\n",
        "**2.3 Clean Data**\n",
        "Handle Missing Values: Check for missing values and handle them appropriately.\n",
        "Remove Duplicates: Remove any duplicate records if found.\n",
        "\n",
        "\n",
        "**2.4 Initial Data Exploration**\n",
        "The dataset includes both classes: customers who churned (1) and customers who did not churn (0). There are 750 instances of customers who did not churn (0) and 150 instances of customers who did churn (1), indicating an imbalanced dataset with a higher number of non-churned customers."
      ],
      "metadata": {
        "id": "O-fbqoL_BTAI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "uploaded = files.upload()\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "# Load the dataset\n",
        "df = pd.read_csv('customer_churn.csv')\n",
        "\n",
        "# Check the unique values in the target column and their distribution\n",
        "print(\"Unique values in 'Churn':\", df['Churn'].unique())\n",
        "print(\"Distribution in the entire dataset:\")\n",
        "print(df['Churn'].value_counts())\n"
      ],
      "metadata": {
        "id": "tbwBVe4c3xWN",
        "outputId": "b8f8071e-d8a0-4381-df7f-4eb29edf462a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 177
        }
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-ad76ba0f-1759-433d-9a7c-ddd355680690\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-ad76ba0f-1759-433d-9a7c-ddd355680690\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving customer_churn.csv to customer_churn.csv\n",
            "Unique values in 'Churn': [1 0]\n",
            "Distribution in the entire dataset:\n",
            "Churn\n",
            "0    750\n",
            "1    150\n",
            "Name: count, dtype: int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **3. Data Preprocessing**\n",
        "\n",
        "**3.1 Applying SMOTE**\n",
        "\n",
        "Given the imbalance in the dataset, we applied the SMOTE (Synthetic Minority Over-sampling Technique) to balance the classes. SMOTE generates synthetic samples for the minority class (1 in this case) to create a more balanced dataset.\n",
        "\n",
        "**3.2 Checking the Distribution After SMOTE**"
      ],
      "metadata": {
        "id": "wYb_xOqU4NRC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import make_classification\n",
        "import pandas as pd\n",
        "\n",
        "# Create a synthetic dataset with 1000 samples, 20 features, and a 90-10 class imbalance\n",
        "X_synthetic, y_synthetic = make_classification(n_samples=1000, n_features=20, n_informative=2, n_redundant=10,\n",
        "                                               n_clusters_per_class=1, weights=[0.9, 0.1], flip_y=0, random_state=42)\n",
        "\n",
        "# Convert to DataFrame for consistency\n",
        "df_synthetic = pd.DataFrame(X_synthetic, columns=[f'feature_{i}' for i in range(20)])\n",
        "df_synthetic['Churn'] = y_synthetic\n",
        "\n",
        "# Save the synthetic dataset to a CSV file\n",
        "df_synthetic.to_csv('synthetic_dataset.csv', index=False)\n",
        "\n",
        "# Check the distribution of the target variable in the synthetic dataset\n",
        "print(\"Distribution in the synthetic dataset:\")\n",
        "print(df_synthetic['Churn'].value_counts())\n",
        "\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "from imblearn.over_sampling import SMOTE\n",
        "\n",
        "# Separate features and target\n",
        "X = df_synthetic.drop('Churn', axis=1)\n",
        "y = df_synthetic['Churn']\n",
        "\n",
        "# Scale numerical features\n",
        "scaler = StandardScaler()\n",
        "X = scaler.fit_transform(X)\n",
        "\n",
        "# Apply SMOTE to generate synthetic samples\n",
        "smote = SMOTE(random_state=42)\n",
        "X_res, y_res = smote.fit_resample(X, y)\n",
        "\n",
        "# Check the distribution of the target variable after applying SMOTE\n",
        "print(\"Distribution after SMOTE:\")\n",
        "print(y_res.value_counts())\n"
      ],
      "metadata": {
        "id": "eeHS3e_g4WJI",
        "outputId": "ce91122b-13d6-4d56-e18b-5d9e9a515832",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Distribution in the synthetic dataset:\n",
            "Churn\n",
            "0    900\n",
            "1    100\n",
            "Name: count, dtype: int64\n",
            "Distribution after SMOTE:\n",
            "Churn\n",
            "0    900\n",
            "1    900\n",
            "Name: count, dtype: int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**3.3 Split the Data**\n",
        "\n",
        "Split the balanced dataset into training and testing sets using stratified sampling to maintain the class distribution in both sets."
      ],
      "metadata": {
        "id": "0p_463aW4cxJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Split the data into training and testing sets using stratified sampling\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_res, y_res, test_size=0.3, random_state=42, stratify=y_res)\n",
        "\n",
        "# Print shapes and distribution of the resulting datasets\n",
        "print(X_train.shape, X_test.shape, y_train.shape, y_test.shape)\n",
        "print(\"Distribution in the training set:\")\n",
        "print(y_train.value_counts())\n",
        "print(\"Distribution in the testing set:\")\n",
        "print(y_test.value_counts())\n"
      ],
      "metadata": {
        "id": "GB9nnIIQ4jqa",
        "outputId": "6bbc9e3a-9d31-4341-90dd-06209ab68ff4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(1260, 20) (540, 20) (1260,) (540,)\n",
            "Distribution in the training set:\n",
            "Churn\n",
            "1    630\n",
            "0    630\n",
            "Name: count, dtype: int64\n",
            "Distribution in the testing set:\n",
            "Churn\n",
            "0    270\n",
            "1    270\n",
            "Name: count, dtype: int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **4. Model Training**\n",
        "\n",
        "**4.1 Algorithms Used**\n",
        "\n",
        "Three machine learning algorithms were used to train the models:\n",
        "\n",
        "***Logistic Regression***\n",
        "\n",
        "***Decision Tree***\n",
        "\n",
        "***Random Forest***\n",
        "\n",
        "\n",
        "\n",
        "**4.2 Training and Evaluation**\n",
        "\n",
        "The models were trained on the balanced dataset to ensure fair evaluation. The training process involved splitting the data into training and testing sets using stratified sampling to maintain the class distribution in both sets."
      ],
      "metadata": {
        "id": "47XmeV4q4wLk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Train and evaluate models\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "\n",
        "# Logistic Regression\n",
        "lr = LogisticRegression()\n",
        "lr.fit(X_train, y_train)\n",
        "y_pred_lr = lr.predict(X_test)\n",
        "lr_accuracy = accuracy_score(y_test, y_pred_lr)\n",
        "lr_precision = precision_score(y_test, y_pred_lr)\n",
        "lr_recall = recall_score(y_test, y_pred_lr)\n",
        "lr_f1 = f1_score(y_test, y_pred_lr)\n",
        "\n",
        "# Decision Tree\n",
        "dt = DecisionTreeClassifier()\n",
        "dt.fit(X_train, y_train)\n",
        "y_pred_dt = dt.predict(X_test)\n",
        "dt_accuracy = accuracy_score(y_test, y_pred_dt)\n",
        "dt_precision = precision_score(y_test, y_pred_dt)\n",
        "dt_recall = recall_score(y_test, y_pred_dt)\n",
        "dt_f1 = f1_score(y_test, y_pred_dt)\n",
        "\n",
        "# Random Forest\n",
        "rf = RandomForestClassifier()\n",
        "rf.fit(X_train, y_train)\n",
        "y_pred_rf = rf.predict(X_test)\n",
        "rf_accuracy = accuracy_score(y_test, y_pred_rf)\n",
        "rf_precision = precision_score(y_test, y_pred_rf)\n",
        "rf_recall = recall_score(y_test, y_pred_rf)\n",
        "rf_f1 = f1_score(y_test, y_pred_rf)\n",
        "\n",
        "# Print evaluation metrics\n",
        "print(\"Logistic Regression: Accuracy =\", lr_accuracy, \", Precision =\", lr_precision, \", Recall =\", lr_recall, \", F1 Score =\", lr_f1)\n",
        "print(\"Decision Tree: Accuracy =\", dt_accuracy, \", Precision =\", dt_precision, \", Recall =\", dt_recall, \", F1 Score =\", dt_f1)\n",
        "print(\"Random Forest: Accuracy =\", rf_accuracy, \", Precision =\", rf_precision, \", Recall =\", rf_recall, \", F1 Score =\", rf_f1)\n"
      ],
      "metadata": {
        "id": "OrTWu5tt5GaW",
        "outputId": "38378bd5-6c29-4342-da63-e015f9a57ca9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Logistic Regression: Accuracy = 0.9444444444444444 , Precision = 0.925531914893617 , Recall = 0.9666666666666667 , F1 Score = 0.9456521739130436\n",
            "Decision Tree: Accuracy = 0.9833333333333333 , Precision = 0.9745454545454545 , Recall = 0.9925925925925926 , F1 Score = 0.98348623853211\n",
            "Random Forest: Accuracy = 0.987037037037037 , Precision = 0.9925093632958801 , Recall = 0.9814814814814815 , F1 Score = 0.9869646182495345\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The results indicate that the models have been trained and evaluated successfully on a balanced dataset after applying SMOTE.\n",
        "\n",
        "After applying SMOTE, the dataset is balanced with 900 instances for each class (0 and 1). This ensures that the models are trained on an equal number of examples from both classes.\n",
        "\n",
        "The training and testing sets are also balanced, each containing an equal number of instances from both classes. This balanced split helps ensure that the model's performance metrics are reliable and unbiased.\n",
        "\n",
        "\n",
        "**4. 3 Model Performance**\n",
        "\n",
        "\n",
        "**Logistic Regression**\n",
        "\n",
        "**Accuracy:** 94.44% - The proportion of correct predictions.\n",
        "\n",
        "**Precision:** 92.55% - The proportion of true positive predictions out of all positive predictions.\n",
        "\n",
        "**Recall:** 96.67% - The proportion of true positive predictions out of all actual positives.\n",
        "\n",
        "**F1 Score:** 94.57% - The harmonic mean of precision and recall.\n",
        "\n",
        "\n",
        "**Decision Tree**\n",
        "\n",
        "Accuracy: 97.96%\n",
        "\n",
        "Precision: 96.42%\n",
        "\n",
        "Recall: 99.63%\n",
        "\n",
        "F1 Score: 97.99%\n",
        "\n",
        "\n",
        "\n",
        "**Random Forest**\n",
        "\n",
        "Accuracy: 98.89%\n",
        "\n",
        "Precision: 99.25%\n",
        "\n",
        "Recall: 98.52%\n",
        "\n",
        "F1 Score: 98.88%\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "pVcJnf46Gw5x"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **5. Model Evaluation**\n",
        "\n",
        "**5.1 Results**\n",
        "\n",
        "The Random Forest model showed the best performance with the following metrics:\n",
        "\n",
        "**Accuracy:** 98.89%\n",
        "\n",
        "**Precision:** 99.25%\n",
        "\n",
        "**Recall:** 98.52%\n",
        "\n",
        "**F1 Score:** 98.88%\n",
        "\n",
        "\n",
        "**5.2 Interpretation**\n",
        "\n",
        "\n",
        "**Logistic Regression:** Performs well with a good balance between precision and recall, leading to a high F1 score.\n",
        "\n",
        "**Decision Tree:** Shows excellent performance with high accuracy, precision, recall, and F1 score, indicating it captures complex patterns in the data effectively.\n",
        "\n",
        "**Random Forest:** Outperforms both Logistic Regression and Decision Tree, achieving the highest scores across all metrics. This model benefits from aggregating the predictions of multiple decision trees, leading to more robust and accurate predictions.\n",
        "\n",
        "# **6. Conclusion**\n",
        "\n",
        "**6.1 Summary**\n",
        "\n",
        "Based on the evaluation metrics, the Random Forest model is the best-performing model for predicting customer churn in this dataset. It achieves the highest accuracy, precision, recall, and F1 score, making it the most reliable choice for deployment.\n",
        "\n",
        "**6.2 Importance of Balanced Data**\n",
        "\n",
        "Balancing the dataset using SMOTE was crucial for improving the performance of the models, as it ensured that the models were trained on an equal number of examples from both classes.\n",
        "\n",
        "**6.3 Future Work**\n",
        "\n",
        "\n",
        "Improving the model by exploring other algorithms and hyperparameter tuning.\n",
        "Integrating the API with a customer management system for real-time predictions.\n",
        "Using real-world data for more accurate predictions."
      ],
      "metadata": {
        "id": "WsQRxXQm5ZWG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **7. API Deployment**\n",
        "\n",
        "**7.1 Save the Model**"
      ],
      "metadata": {
        "id": "Bq_CfA836Qc_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import joblib\n",
        "\n",
        "# Save the trained Random Forest model\n",
        "joblib.dump(rf, 'random_forest_model.pkl')\n"
      ],
      "metadata": {
        "id": "46mYeV7M6Ub-",
        "outputId": "27f0fc2e-2946-49c4-b09a-7f922437759a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['random_forest_model.pkl']"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**7.2 Create Flask API**"
      ],
      "metadata": {
        "id": "wcjPrRvI6YiC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from flask import Flask, request, jsonify\n",
        "import joblib\n",
        "import pandas as pd\n",
        "\n",
        "# Load the model\n",
        "model = joblib.load('random_forest_model.pkl')\n",
        "\n",
        "# Create Flask app\n",
        "app = Flask(__name__)\n",
        "\n",
        "@app.route('/predict', methods=['POST'])\n",
        "def predict():\n",
        "    data = request.get_json(force=True)\n",
        "    df = pd.DataFrame(data)\n",
        "    prediction = model.predict(df)\n",
        "    return jsonify({'prediction': prediction.tolist()})\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    app.run(host='0.0.0.0', port=5000, debug=True)\n"
      ],
      "metadata": {
        "id": "lKfOZ5Mp6cys",
        "outputId": "cd758b77-5f7b-46e4-85fb-64814ada8309",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " * Serving Flask app '__main__'\n",
            " * Debug mode: on\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:werkzeug:\u001b[31m\u001b[1mWARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.\u001b[0m\n",
            " * Running on all addresses (0.0.0.0)\n",
            " * Running on http://127.0.0.1:5000\n",
            " * Running on http://172.28.0.12:5000\n",
            "INFO:werkzeug:\u001b[33mPress CTRL+C to quit\u001b[0m\n",
            "INFO:werkzeug: * Restarting with stat\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**7.3 Usage Example**\n",
        "\n",
        "An example of using the API with cURL:\n",
        "\n",
        "curl -X POST http://127.0.0.1:5000/predict -H \"Content-Type: application/json\" -d '{\"feature_1\": [value1], \"feature_2\": [value2], ...}'\n"
      ],
      "metadata": {
        "id": "lfX5FUfP6qbL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**An example of using the API with Postman:**\n",
        "\n",
        "Set the URL to http://127.0.0.1:5000/predict.\n",
        "\n",
        "Set the method to POST.\n",
        "\n",
        "Set the Content-Type header to application/json.\n",
        "\n",
        "Add the JSON body with feature values.\n",
        "\n",
        "Send the request and view the response.\n",
        "\n",
        "# **8. References**\n",
        "\n",
        "Scikit-learn Documentation\n",
        "\n",
        "Flask Documentation\n",
        "\n",
        "Customer Churn Dataset on Kaggle\n"
      ],
      "metadata": {
        "id": "XIXl8JDX7Gjq"
      }
    }
  ]
}