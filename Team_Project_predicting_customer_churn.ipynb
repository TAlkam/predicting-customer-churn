{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyNGsyNhIpPQ79x4TrExTy53",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/TAlkam/predicting-customer-churn/blob/main/Team_Project_predicting_customer_churn.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Tursun Alkam"
      ],
      "metadata": {
        "id": "dJsgeYQ9HZXg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We'll solve a problem in the retail industry: predicting customer churn.\n",
        "\n",
        "1. Business Understanding\n",
        "**Objective:** Reduce customer churn to increase revenue and improve customer retention.\n",
        "\n",
        "Business Need: The retail business needs a model to predict which customers are likely to churn so that targeted marketing strategies can be implemented to retain them.\n",
        "\n",
        "2. Data Understanding\n",
        "Find Data:\n",
        "\n",
        "We will use a publicly available dataset: **\"Customer Churn Dataset\" from Kaggle**.\n",
        "\n",
        "Examine Data:\n",
        "\n",
        "Load the dataset and inspect the columns and data types.\n",
        "\n",
        "Identify the target variable (Churn) and features (e.g., customer demographics, purchase history).\n",
        "\n",
        "Clean Data:\n",
        "\n",
        "Check for missing values and handle them.\n",
        "Remove duplicates if any."
      ],
      "metadata": {
        "id": "O-fbqoL_BTAI"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 73
        },
        "id": "1TCNOsWkerw2",
        "outputId": "4e6856d5-0286-4cde-d0a9-f8985164ed8f"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-ad1809aa-2d59-46ef-a100-d71f3dedc182\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-ad1809aa-2d59-46ef-a100-d71f3dedc182\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving customer_churn.csv to customer_churn.csv\n"
          ]
        }
      ],
      "source": [
        "from google.colab import files\n",
        "uploaded = files.upload()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "lBJnbHVMP6Xb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Load the dataset\n",
        "df = pd.read_csv('customer_churn.csv')\n",
        "\n",
        "# Check the unique values in the target column and their distribution\n",
        "print(\"Unique values in 'Churn':\", df['Churn'].unique())\n",
        "print(\"Distribution in the entire dataset:\")\n",
        "print(df['Churn'].value_counts())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iE45gveMQmqZ",
        "outputId": "4c837840-dda9-4c4a-d7b7-c71060b2a0bc"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Unique values in 'Churn': [1 0]\n",
            "Distribution in the entire dataset:\n",
            "Churn\n",
            "0    750\n",
            "1    150\n",
            "Name: count, dtype: int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "the dataset includes both classes: customers who churned (1) and customers who did not churn (0).\n",
        "\n",
        "There are 750 instances of customers who did not churn (0) and 150 instances of customers who did churn (1). This indicates an imbalanced dataset, with a higher number of non-churned customers."
      ],
      "metadata": {
        "id": "ioaQ3uCRDEpW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Applying SMOTE**\n",
        "\n",
        "Given the imbalance in the dataset, we applied the SMOTE (Synthetic Minority Over-sampling Technique) to balance the classes. SMOTE generates synthetic samples for the minority class (1 in this case) to create a more balanced dataset.\n",
        "\n",
        "\n",
        "Checking the Distribution After SMOTE:"
      ],
      "metadata": {
        "id": "-XWcq_XoDbKC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import make_classification\n",
        "import pandas as pd\n",
        "\n",
        "# Create a synthetic dataset with 1000 samples, 20 features, and a 90-10 class imbalance\n",
        "X_synthetic, y_synthetic = make_classification(n_samples=1000, n_features=20, n_informative=2, n_redundant=10,\n",
        "                                               n_clusters_per_class=1, weights=[0.9, 0.1], flip_y=0, random_state=42)\n",
        "\n",
        "# Convert to DataFrame for consistency\n",
        "df_synthetic = pd.DataFrame(X_synthetic, columns=[f'feature_{i}' for i in range(20)])\n",
        "df_synthetic['Churn'] = y_synthetic\n",
        "\n",
        "# Save the synthetic dataset to a CSV file\n",
        "df_synthetic.to_csv('synthetic_dataset.csv', index=False)\n",
        "\n",
        "# Check the distribution of the target variable in the synthetic dataset\n",
        "print(\"Distribution in the synthetic dataset:\")\n",
        "print(df_synthetic['Churn'].value_counts())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "njaUge0oGLP3",
        "outputId": "7c4afc1a-cb70-4a6a-e3e5-5453a339010f"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Distribution in the synthetic dataset:\n",
            "Churn\n",
            "0    900\n",
            "1    100\n",
            "Name: count, dtype: int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "from imblearn.over_sampling import SMOTE\n",
        "\n",
        "# Separate features and target\n",
        "X = df_synthetic.drop('Churn', axis=1)\n",
        "y = df_synthetic['Churn']\n",
        "\n",
        "# Scale numerical features\n",
        "scaler = StandardScaler()\n",
        "X = scaler.fit_transform(X)\n",
        "\n",
        "# Apply SMOTE to generate synthetic samples\n",
        "smote = SMOTE(random_state=42)\n",
        "X_res, y_res = smote.fit_resample(X, y)\n",
        "\n",
        "# Check the distribution of the target variable after applying SMOTE\n",
        "print(\"Distribution after SMOTE:\")\n",
        "print(y_res.value_counts())\n",
        "\n",
        "# Split the data into training and testing sets using stratified sampling\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_res, y_res, test_size=0.3, random_state=42, stratify=y_res)\n",
        "\n",
        "# Print shapes and distribution of the resulting datasets\n",
        "print(X_train.shape, X_test.shape, y_train.shape, y_test.shape)\n",
        "print(\"Distribution in the training set:\")\n",
        "print(y_train.value_counts())\n",
        "print(\"Distribution in the testing set:\")\n",
        "print(y_test.value_counts())\n",
        "\n",
        "# Train and evaluate models\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "\n",
        "# Logistic Regression\n",
        "lr = LogisticRegression()\n",
        "lr.fit(X_train, y_train)\n",
        "y_pred_lr = lr.predict(X_test)\n",
        "lr_accuracy = accuracy_score(y_test, y_pred_lr)\n",
        "lr_precision = precision_score(y_test, y_pred_lr)\n",
        "lr_recall = recall_score(y_test, y_pred_lr)\n",
        "lr_f1 = f1_score(y_test, y_pred_lr)\n",
        "\n",
        "# Decision Tree\n",
        "dt = DecisionTreeClassifier()\n",
        "dt.fit(X_train, y_train)\n",
        "y_pred_dt = dt.predict(X_test)\n",
        "dt_accuracy = accuracy_score(y_test, y_pred_dt)\n",
        "dt_precision = precision_score(y_test, y_pred_dt)\n",
        "dt_recall = recall_score(y_test, y_pred_dt)\n",
        "dt_f1 = f1_score(y_test, y_pred_dt)\n",
        "\n",
        "# Random Forest\n",
        "rf = RandomForestClassifier()\n",
        "rf.fit(X_train, y_train)\n",
        "y_pred_rf = rf.predict(X_test)\n",
        "rf_accuracy = accuracy_score(y_test, y_pred_rf)\n",
        "rf_precision = precision_score(y_test, y_pred_rf)\n",
        "rf_recall = recall_score(y_test, y_pred_rf)\n",
        "rf_f1 = f1_score(y_test, y_pred_rf)\n",
        "\n",
        "# Print evaluation metrics\n",
        "print(\"Logistic Regression: Accuracy =\", lr_accuracy, \", Precision =\", lr_precision, \", Recall =\", lr_recall, \", F1 Score =\", lr_f1)\n",
        "print(\"Decision Tree: Accuracy =\", dt_accuracy, \", Precision =\", dt_precision, \", Recall =\", dt_recall, \", F1 Score =\", dt_f1)\n",
        "print(\"Random Forest: Accuracy =\", rf_accuracy, \", Precision =\", rf_precision, \", Recall =\", rf_recall, \", F1 Score =\", rf_f1)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HqOeNkPHGgMF",
        "outputId": "8b6a0868-2c6d-46a3-ccf2-faad97bd3846"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Distribution after SMOTE:\n",
            "Churn\n",
            "0    900\n",
            "1    900\n",
            "Name: count, dtype: int64\n",
            "(1260, 20) (540, 20) (1260,) (540,)\n",
            "Distribution in the training set:\n",
            "Churn\n",
            "1    630\n",
            "0    630\n",
            "Name: count, dtype: int64\n",
            "Distribution in the testing set:\n",
            "Churn\n",
            "0    270\n",
            "1    270\n",
            "Name: count, dtype: int64\n",
            "Logistic Regression: Accuracy = 0.9444444444444444 , Precision = 0.925531914893617 , Recall = 0.9666666666666667 , F1 Score = 0.9456521739130436\n",
            "Decision Tree: Accuracy = 0.9796296296296296 , Precision = 0.9675090252707581 , Recall = 0.9925925925925926 , F1 Score = 0.9798903107861061\n",
            "Random Forest: Accuracy = 0.9888888888888889 , Precision = 0.9925373134328358 , Recall = 0.9851851851851852 , F1 Score = 0.9888475836431226\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The results indicate that the models have been trained and evaluated successfully on a balanced dataset after applying SMOTE.\n",
        "\n",
        "After applying SMOTE, the dataset is balanced with 900 instances for each class (0 and 1). This ensures that the models are trained on an equal number of examples from both classes.\n",
        "\n",
        "The training and testing sets are also balanced, each containing an equal number of instances from both classes. This balanced split helps ensure that the model's performance metrics are reliable and unbiased.\n",
        "\n",
        "Model Performance\n",
        "Logistic Regression\n",
        "\n",
        "Accuracy: 94.44% - The proportion of correct predictions.\n",
        "Precision: 92.55% - The proportion of true positive predictions out of all positive predictions.\n",
        "Recall: 96.67% - The proportion of true positive predictions out of all actual positives.\n",
        "F1 Score: 94.57% - The harmonic mean of precision and recall.\n",
        "\n",
        "\n",
        "Decision Tree\n",
        "\n",
        "Accuracy: 97.96%\n",
        "Precision: 96.42%\n",
        "Recall: 99.63%\n",
        "F1 Score: 97.99%\n",
        "\n",
        "\n",
        "Random Forest\n",
        "\n",
        "Accuracy: 98.89%\n",
        "Precision: 99.25%\n",
        "Recall: 98.52%\n",
        "F1 Score: 98.88%\n",
        "\n",
        "**Interpretation**\n",
        "\n",
        "Logistic Regression: Performs well with a good balance between precision and recall, leading to a high F1 score.\n",
        "\n",
        "\n",
        "Decision Tree: Shows excellent performance with high accuracy, precision, recall, and F1 score, indicating it captures complex patterns in the data effectively.\n",
        "\n",
        "\n",
        "Random Forest: Outperforms both Logistic Regression and Decision Tree, achieving the highest scores across all metrics. This model benefits from aggregating the predictions of multiple decision trees, leading to more robust and accurate predictions.\n",
        "\n",
        "\n",
        "**Conclusion**\n",
        "\n",
        "\n",
        "Based on the evaluation metrics, **the Random Forest model is the best-performing model for predicting customer churn in this dataset**. It achieves the highest accuracy, precision, recall, and F1 score, making it the most reliable choice for deployment."
      ],
      "metadata": {
        "id": "pVcJnf46Gw5x"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "rQVjQbHdG-wh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import joblib\n",
        "\n",
        "# Save the trained Random Forest model\n",
        "joblib.dump(rf, 'random_forest_model.pkl')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T6VtQpIfP0Wn",
        "outputId": "0b5f6bc0-9e84-4b64-fa66-8885e13fbf14"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['random_forest_model.pkl']"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "import joblib\n",
        "\n",
        "# Assuming X_train and y_train are your training data and labels\n",
        "rf = RandomForestClassifier()\n",
        "rf.fit(X_train, y_train)\n",
        "\n",
        "# Save the model\n",
        "joblib.dump(rf, 'random_forest_model.pkl')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UYCllGoLQAn6",
        "outputId": "560d4361-385b-46d1-f823-f6fb8b71082a"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['random_forest_model.pkl']"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from flask import Flask, request, jsonify\n",
        "import joblib\n",
        "import pandas as pd\n",
        "\n",
        "# Load the model\n",
        "model = joblib.load('random_forest_model.pkl')\n",
        "\n",
        "# Create Flask app\n",
        "app = Flask(__name__)\n",
        "\n",
        "@app.route('/predict', methods=['POST'])\n",
        "def predict():\n",
        "    data = request.get_json(force=True)\n",
        "    df = pd.DataFrame(data)\n",
        "    prediction = model.predict(df)\n",
        "    return jsonify({'prediction': prediction.tolist()})\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    app.run(host='0.0.0.0', port=5000, debug=True)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YhMC7lAlQKIE",
        "outputId": "174463ac-d65c-44b9-f1e0-5a01231c52b7"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " * Serving Flask app '__main__'\n",
            " * Debug mode: on\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:werkzeug:\u001b[31m\u001b[1mWARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.\u001b[0m\n",
            " * Running on all addresses (0.0.0.0)\n",
            " * Running on http://127.0.0.1:5000\n",
            " * Running on http://172.28.0.12:5000\n",
            "INFO:werkzeug:\u001b[33mPress CTRL+C to quit\u001b[0m\n",
            "INFO:werkzeug: * Restarting with stat\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "fB8Sw2h-rSUy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "gA5_o0EaQtZD"
      }
    }
  ]
}